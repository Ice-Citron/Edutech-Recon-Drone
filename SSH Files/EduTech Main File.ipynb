{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install roboflow\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IbVyatAx5S4",
        "outputId": "79fb6433-0098-4a6a-8134-983955800305"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.18-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.18-py3-none-any.whl (876 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m876.6/876.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.18 ultralytics-thop-2.0.9\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.48-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Downloading roboflow-1.1.48-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, idna, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.48\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.16.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "IY0KJ_5ulgBn",
        "outputId": "e4e32bc1-64f5-4ef1-f977-16c8aa5783b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination directory: /content/datasets\n",
            "Processing sea-ezx3q...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 44 of sea-ezx3q\n",
            "Successfully downloaded sea-ezx3q to /content/datasets/sea-ezx3q\n",
            "Contents of /content/datasets/sea-ezx3q: ['test', 'README.dataset.txt', 'valid', 'data.yaml', 'train', 'README.roboflow.txt']\n",
            "--------------------------------------------------\n",
            "Processing marine-debris-vb2qm...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 16 of marine-debris-vb2qm\n",
            "Successfully downloaded marine-debris-vb2qm to /content/datasets/marine-debris-vb2qm\n",
            "Contents of /content/datasets/marine-debris-vb2qm: ['test', 'README.dataset.txt', 'valid', 'data.yaml', 'train', 'README.roboflow.txt']\n",
            "--------------------------------------------------\n",
            "Processing new-taco...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 3 of new-taco\n",
            "Successfully downloaded new-taco to /content/datasets/new-taco\n",
            "Contents of /content/datasets/new-taco: ['test', 'README.dataset.txt', 'valid', 'data.yaml', 'train', 'README.roboflow.txt']\n",
            "--------------------------------------------------\n",
            "Processing -kg...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 5 of -kg\n",
            "Successfully downloaded -kg to /content/datasets/-kg\n",
            "Contents of /content/datasets/-kg: ['test', 'README.dataset.txt', 'valid', 'data.yaml', 'train', 'README.roboflow.txt']\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9b56a5db0bf0>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {dataset_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "rf = Roboflow(api_key=\"TZCi2SCltMQkbaVAtOqT\")\n",
        "\n",
        "# List of datasets to download (workspace, dataset, version)\n",
        "datasets_info = [\n",
        "    (\"hongmo\", \"sea-ezx3q\", 44),\n",
        "    (\"kim-borma\", \"marine-debris-vb2qm\", 16),\n",
        "    (\"myfyp\", \"new-taco\", 3),\n",
        "    (\"anas-fd5bu\", \"-kg\", 5),\n",
        "\"\"\"\n",
        "    (\"trash-detection-eka7g\", \"aeriel-trash-detection\", 5),\n",
        "    (\"fyp-bfx3h\", \"yolov8-trash-detections\", 6),\n",
        "    (\"taco-ihjgk\", \"yolov8-trash-detections-kgnug\", 11),\n",
        "    (\"trash-ai-27vay\", \"ultimate-rqkdd\", 1),\n",
        "    (\"iqram\", \"trashdetectionv8\", 2),\n",
        "    (\"smart-india-hackathon-2023\", \"garbage_best\", 1),\n",
        "    # New datasets (replace VERSION with the correct version number)\n",
        "    (\"kim-borma\", \"conttttt\", 1),\n",
        "    (\"adamson-university-rkm6w\", \"manila-bay\", 3),\n",
        "    (\"davids-workspace\", \"garbage-detection-mnj17\", 1),\n",
        "    (\"majasworkspace\", \"p_eine_klasse\", 1),\n",
        "    (\"ukm-wcn\", \"ml2-wcn-ukm\", 23),\n",
        "    (\"vit-akped\", \"tacodetecron-2\", 1),\n",
        "    (\"k2k\", \"imported-f38bi\", 4),\n",
        "    (\"python-project-5oizd\", \"trash-detection-mzljs\", 7),\n",
        "    (\"project-mqoot\", \"plastic-bag-detection\", 1),\n",
        "    (\"trash-dataset-for-oriented-bounded-box\", \"trash-detection-1fjjc\", 14),\n",
        "    (\"nam-nhat\", \"trash-dvdrr\", 5),\n",
        "    (\"litter-beach-detection\", \"beach-garbage\", 2),\n",
        "    (\"dechets-qlhbp\", \"tacoo-cj6rf\", 1),\n",
        "    (\"cscsi218\", \"garbageclassification-47d7t\", 1),\n",
        "    (\"hust-xz9js\", \"trashbot-wahhb\", 1),\n",
        "    (\"furqan-sayyed-veuct\", \"taco-ppujr\", 1),\n",
        "    (\"abhijeet-beedikar-pbc0x\", \"garbage-classification-taco\", 1)\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "dest_dir = os.path.abspath(\"datasets\")\n",
        "print(f\"Destination directory: {dest_dir}\")\n",
        "\n",
        "if not os.path.exists(dest_dir):\n",
        "    os.makedirs(dest_dir)\n",
        "\n",
        "for workspace, dataset_name, version in datasets_info:\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "    try:\n",
        "        project = rf.workspace(workspace).project(dataset_name)\n",
        "        print(f\"Attempting to download version {version} of {dataset_name}\")\n",
        "\n",
        "        # Specify the download location\n",
        "        download_dir = os.path.join(dest_dir, dataset_name)\n",
        "\n",
        "        # Download the dataset to the specified location\n",
        "        dataset = project.version(version).download(\"yolov11\", location=download_dir)\n",
        "        print(f\"Successfully downloaded {dataset_name} to {dataset.location}\")\n",
        "\n",
        "        # List the contents of the downloaded directory\n",
        "        if os.path.exists(dataset.location):\n",
        "            print(f\"Contents of {dataset.location}: {os.listdir(dataset.location)}\")\n",
        "        else:\n",
        "            print(f\"Warning: Downloaded directory not found at {dataset.location}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dataset_name}: {str(e)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Download process completed. Please check the following directory for your files:\")\n",
        "print(dest_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import glob\n",
        "\n",
        "def merge_datasets(datasets_dir, combined_dataset_dir):\n",
        "    # Create directories for the combined dataset\n",
        "    os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "    splits = ['train', 'valid', 'test']\n",
        "    for split in splits:\n",
        "        os.makedirs(os.path.join(combined_dataset_dir, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(combined_dataset_dir, 'labels', split), exist_ok=True)\n",
        "\n",
        "    combined_classes = []\n",
        "    class_mappings = {}\n",
        "\n",
        "    # First pass: Collect all class names and build combined_classes list\n",
        "    for dataset_name in os.listdir(datasets_dir):\n",
        "        dataset_path = os.path.join(datasets_dir, dataset_name)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "        if not os.path.exists(data_yaml_path):\n",
        "            print(f\"data.yaml not found in {dataset_path}. Skipping this dataset.\")\n",
        "            continue\n",
        "\n",
        "        with open(data_yaml_path, 'r') as f:\n",
        "            data_yaml = yaml.safe_load(f)\n",
        "\n",
        "        dataset_classes = data_yaml.get('names', [])\n",
        "        if not dataset_classes:\n",
        "            print(f\"No 'names' field found in {data_yaml_path}. Skipping this dataset.\")\n",
        "            continue\n",
        "\n",
        "        # Update combined_classes and create class mappings for this dataset\n",
        "        mapping = {}\n",
        "        for idx, class_name in enumerate(dataset_classes):\n",
        "            if class_name not in combined_classes:\n",
        "                combined_classes.append(class_name)\n",
        "            mapping[idx] = combined_classes.index(class_name)\n",
        "        class_mappings[dataset_name] = mapping\n",
        "\n",
        "    print(f\"Combined classes: {combined_classes}\")\n",
        "\n",
        "    # Second pass: Copy images and labels, remap class IDs\n",
        "    for dataset_name in os.listdir(datasets_dir):\n",
        "        dataset_path = os.path.join(datasets_dir, dataset_name)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        mapping = class_mappings.get(dataset_name)\n",
        "        if mapping is None:\n",
        "            continue  # Skip datasets that were not included in combined_classes\n",
        "\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            split_dir = os.path.join(dataset_path, split)\n",
        "            if not os.path.exists(split_dir):\n",
        "                print(f\"Split '{split}' not found in {dataset_name}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Check if images and labels are under 'images' and 'labels' directories\n",
        "            images_dir = os.path.join(split_dir, 'images')\n",
        "            labels_dir = os.path.join(split_dir, 'labels')\n",
        "            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
        "                # Maybe images and labels are directly under split_dir\n",
        "                images_dir = split_dir\n",
        "                labels_dir = split_dir\n",
        "                # Check if there are images and labels in split_dir\n",
        "                image_files = glob.glob(os.path.join(images_dir, '*.jpg')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.jpeg')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.png')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.bmp')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.tif')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.tiff'))\n",
        "                label_files = glob.glob(os.path.join(labels_dir, '*.txt'))\n",
        "                if len(image_files) == 0:\n",
        "                    print(f\"No images found in {images_dir}. Skipping split '{split}' in dataset '{dataset_name}'.\")\n",
        "                    continue\n",
        "            else:\n",
        "                # Images and labels are under 'images' and 'labels' directories\n",
        "                image_files = glob.glob(os.path.join(images_dir, '*.*'))\n",
        "                label_files = glob.glob(os.path.join(labels_dir, '*.txt'))\n",
        "\n",
        "            for image_file in image_files:\n",
        "                image_filename = os.path.basename(image_file)\n",
        "                image_name, image_ext = os.path.splitext(image_filename)\n",
        "\n",
        "                # Corresponding label file\n",
        "                label_file = os.path.join(labels_dir, f\"{image_name}.txt\")\n",
        "                if not os.path.exists(label_file):\n",
        "                    print(f\"Label file {label_file} not found. Creating empty label.\")\n",
        "                    label_lines = []\n",
        "                else:\n",
        "                    with open(label_file, 'r') as lf:\n",
        "                        label_lines = lf.readlines()\n",
        "\n",
        "                # Remap class IDs in label file\n",
        "                remapped_labels = []\n",
        "                for line in label_lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:\n",
        "                        print(f\"Invalid label format in {label_file}. Skipping line.\")\n",
        "                        continue\n",
        "                    try:\n",
        "                        class_id = int(parts[0])\n",
        "                        new_class_id = mapping.get(class_id)\n",
        "                        if new_class_id is None:\n",
        "                            print(f\"Class ID {class_id} not found in mapping for {dataset_name}. Skipping line.\")\n",
        "                            continue\n",
        "                        parts[0] = str(new_class_id)\n",
        "                        remapped_labels.append(' '.join(parts))\n",
        "                    except ValueError:\n",
        "                        print(f\"Invalid class ID in {label_file}. Skipping line.\")\n",
        "                        continue\n",
        "\n",
        "                # Copy image\n",
        "                dest_image_name = f\"{dataset_name}_{image_filename}\"\n",
        "                target_split = split  # Use the same split name\n",
        "                dest_image_path = os.path.join(combined_dataset_dir, 'images', target_split, dest_image_name)\n",
        "                shutil.copy(image_file, dest_image_path)\n",
        "\n",
        "                # Write remapped label\n",
        "                dest_label_name = f\"{dataset_name}_{image_name}.txt\"\n",
        "                dest_label_path = os.path.join(combined_dataset_dir, 'labels', target_split, dest_label_name)\n",
        "                with open(dest_label_path, 'w') as lf:\n",
        "                    lf.write('\\n'.join(remapped_labels))\n",
        "\n",
        "    # Create combined data.yaml\n",
        "    combined_data = {\n",
        "        'path': os.path.abspath(combined_dataset_dir),\n",
        "        'train': os.path.join('images', 'train'),\n",
        "        'val': os.path.join('images', 'valid'),  # Use 'valid' as validation set\n",
        "        'names': combined_classes\n",
        "    }\n",
        "    with open(os.path.join(combined_dataset_dir, 'data.yaml'), 'w') as f:\n",
        "        yaml.dump(combined_data, f)\n",
        "\n",
        "    print(\"Dataset merging complete.\")\n",
        "\n",
        "# Usage\n",
        "datasets_dir = 'datasets'  # Directory containing individual datasets\n",
        "combined_dataset_dir = 'combined_dataset'  # Directory to store the combined dataset\n",
        "\n",
        "merge_datasets(datasets_dir, combined_dataset_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01G3t6pRx__z",
        "outputId": "7b92789a-4831-49cd-b47c-abeb4f884066"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined classes: ['Aluminium foil', 'Bottle', 'Bottle cap', 'Can', 'Carton', 'Cigarette', 'Glass bottle', 'Lid', 'Metal', 'Metal cap', 'Other litter', 'Other plastic', 'Paper', 'Plastic bag', 'Plastic buoy', 'Plastic vessels', 'Pop tab', 'Straw', 'Styrofoam cup', 'Styrofoam piece', 'Styrofoam_Buoy', 'Wrapper', 'Glass', 'Net', 'PET_Bottle', 'Plastic_Buoy', 'Plastic_Buoy_China', 'Plastic_ETC', 'Rope', 'Styrofoam_Box', 'Styrofoam_Piece', 'PET', 'Styrofoam', 'cardboard', 'fabrics', 'glass', 'metal', 'other', 'plastic', 'wood']\n",
            "Dataset merging complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.ultralytics import add_wandb_callback\n",
        "\n",
        "\n",
        "# Set wandb project and run names\n",
        "wandb_project_name = 'Edutech'\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    config={\n",
        "        'epochs': 5,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 0.001,\n",
        "        # Add other hyperparameters if needed\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Load the YOLO11 extra-large detection model\n",
        "model = YOLO('yolo11x.pt')  # Detection model for object detection tasks\n",
        "\n",
        "# Add WandB callback for logging\n",
        "add_wandb_callback(model)\n",
        "\n",
        "\n",
        "# Path to the combined dataset's data.yaml\n",
        "data_yaml_path = os.path.join('combined_dataset', 'data.yaml')\n",
        "\n",
        "# Verify that data.yaml exists\n",
        "if not os.path.exists(data_yaml_path):\n",
        "    raise FileNotFoundError(f\"Error: data.yaml not found at {data_yaml_path}\")\n",
        "else:\n",
        "    print(f\"Using data.yaml at {data_yaml_path}\")\n",
        "\n",
        "# Train the model with wandb logging enabled\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    # name=wandb_run_name,\n",
        "    exist_ok=True,\n",
        "    device='0',\n",
        "    project=wandb_project_name,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pljuUYoix_8Y",
        "outputId": "4f3c2f57-6ab4-4f40-eb08-db12741f2709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m This integration is tested and supported for ultralytics v8.0.238 and below.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m             Please report any issues to https://github.com/wandb/wandb/issues with the tag `yolov8`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241021_164043-qv9gydoe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/Edutech/runs/qv9gydoe' target=\"_blank\">ethereal-sound-16</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/Edutech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/Edutech' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/Edutech</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/Edutech/runs/qv9gydoe' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/Edutech/runs/qv9gydoe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'yolo11x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109M/109M [00:00<00:00, 240MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data.yaml at combined_dataset/data.yaml\n",
            "Ultralytics 8.3.18 ðŸš€ Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11x.pt, data=combined_dataset/data.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=Edutech, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=Edutech/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 101MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=40\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
            "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
            "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
            "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
            " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
            " 17                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
            " 20                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
            " 23        [16, 19, 22]  1   3191752  ultralytics.nn.modules.head.Detect           [40, [384, 768, 768]]         \n",
            "YOLO11x summary: 631 layers, 56,919,976 parameters, 56,919,960 gradients, 195.7 GFLOPs\n",
            "\n",
            "Transferred 1009/1015 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Edutech/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 274MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined_dataset/labels/train... 32090 images, 416 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32090/32090 [00:27<00:00, 1160.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/combined_dataset/images/train/-kg_frame_3069_patch_0_0_jpg.rf.11338fd82fcb4c2d2976c7d82b39339d.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/combined_dataset/images/train/-kg_renamed_DJI_0471_tile_3_7_jpg.rf.987a8f6df1d8eb7780053f92d712e45e.jpg: 1 duplicate labels removed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/combined_dataset/labels/train.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 12132, len(boxes) = 110005. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined_dataset/labels/valid... 6258 images, 135 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6258/6258 [00:05<00:00, 1056.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/combined_dataset/labels/valid.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 3174, len(boxes) = 21792. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to Edutech/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000227, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mEdutech/train\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5      16.6G      1.374      4.093      1.416         87        640:  11%|â–ˆ         | 215/2006 [02:45<22:51,  1.31it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference using the best model\n",
        "results = model.predict(\n",
        "    source=os.path.join(combined_dataset_dir, 'images/val'),\n",
        "    save=True,\n",
        "    project='runs/predict',\n",
        "    name='yolo11x_predict',\n",
        "    exist_ok=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "cuxYBebkx_wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def visualize_predictions(result_dir):\n",
        "    image_paths = glob.glob(os.path.join(result_dir, '*.jpg'))\n",
        "    num_images = min(4, len(image_paths))\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i, image_path in enumerate(image_paths[:num_images]):\n",
        "        image = plt.imread(image_path)\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions('runs/predict/yolo11x_predict')\n"
      ],
      "metadata": {
        "id": "87daXwzIyLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOU1jsLvyLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21RTNNTlyLiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFCIHyOGyLfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
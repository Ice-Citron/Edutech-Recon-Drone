{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install roboflow\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IbVyatAx5S4",
        "outputId": "98df2a07-2661-4c04-f5ba-7094aad973d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.14)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.48)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.8/313.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.16.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"TZCi2SCltMQkbaVAtOqT\")\n",
        "\n",
        "# Replace with a public dataset you have access to\n",
        "workspace = \"hongmo\"\n",
        "dataset_name = \"sea-ezx3q\"\n",
        "\n",
        "try:\n",
        "    project = rf.workspace(workspace).project(dataset_name)\n",
        "    dataset = project.version(44).download(\"yolov8\")\n",
        "    print(f\"Successfully downloaded {dataset_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading {dataset_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZrIQajD30_K",
        "outputId": "8f286d29-2b3c-46c5-c21f-3652c0d5f7aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Successfully downloaded sea-ezx3q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY0KJ_5ulgBn",
        "outputId": "8ce2a35b-03c3-498d-9537-7779b863b002"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination directory: /content/datasets\n",
            "Processing sea-ezx3q...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 44 of sea-ezx3q\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/sea-ezx3q to yolov8:: 100%|██████████| 619562/619562 [00:37<00:00, 16468.29it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/sea-ezx3q in yolov8:: 100%|██████████| 17360/17360 [00:02<00:00, 5827.60it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Successfully downloaded sea-ezx3q to /content/datasets/sea-ezx3q\n",
            "Contents of /content/datasets/sea-ezx3q: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing marine-debris-vb2qm...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 16 of marine-debris-vb2qm\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/marine-debris-vb2qm to yolov8:: 100%|██████████| 592253/592253 [00:36<00:00, 16448.00it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/marine-debris-vb2qm in yolov8:: 100%|██████████| 13828/13828 [00:02<00:00, 5439.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded marine-debris-vb2qm to /content/datasets/marine-debris-vb2qm\n",
            "Contents of /content/datasets/marine-debris-vb2qm: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing new-taco...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 3 of new-taco\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/new-taco to yolov8:: 100%|██████████| 1234406/1234406 [01:10<00:00, 17585.33it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/new-taco in yolov8:: 100%|██████████| 35082/35082 [00:06<00:00, 5836.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded new-taco to /content/datasets/new-taco\n",
            "Contents of /content/datasets/new-taco: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing -kg...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 5 of -kg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/-kg to yolov8:: 100%|██████████| 666657/666657 [00:39<00:00, 16752.02it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/-kg in yolov8:: 100%|██████████| 14774/14774 [00:03<00:00, 4756.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded -kg to /content/datasets/-kg\n",
            "Contents of /content/datasets/-kg: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing aeriel-trash-detection...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 5 of aeriel-trash-detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/aeriel-trash-detection to yolov8:: 100%|██████████| 3165560/3165560 [03:02<00:00, 17319.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/aeriel-trash-detection in yolov8:: 100%|██████████| 61194/61194 [00:12<00:00, 4926.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded aeriel-trash-detection to /content/datasets/aeriel-trash-detection\n",
            "Contents of /content/datasets/aeriel-trash-detection: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing yolov8-trash-detections...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 6 of yolov8-trash-detections\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/yolov8-trash-detections to yolov8:: 100%|██████████| 258946/258946 [00:16<00:00, 15333.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/yolov8-trash-detections in yolov8:: 100%|██████████| 11319/11319 [00:01<00:00, 7209.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded yolov8-trash-detections to /content/datasets/yolov8-trash-detections\n",
            "Contents of /content/datasets/yolov8-trash-detections: ['README.dataset.txt', 'README.roboflow.txt', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing yolov8-trash-detections-kgnug...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 11 of yolov8-trash-detections-kgnug\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/yolov8-trash-detections-kgnug to yolov8:: 100%|██████████| 662965/662965 [00:36<00:00, 18067.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/yolov8-trash-detections-kgnug in yolov8:: 100%|██████████| 24152/24152 [00:03<00:00, 6295.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded yolov8-trash-detections-kgnug to /content/datasets/yolov8-trash-detections-kgnug\n",
            "Contents of /content/datasets/yolov8-trash-detections-kgnug: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing ultimate-rqkdd...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 1 of ultimate-rqkdd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/ultimate-rqkdd to yolov8:: 100%|██████████| 1563128/1563128 [01:28<00:00, 17574.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/ultimate-rqkdd in yolov8:: 100%|██████████| 36696/36696 [00:06<00:00, 5281.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded ultimate-rqkdd to /content/datasets/ultimate-rqkdd\n",
            "Contents of /content/datasets/ultimate-rqkdd: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing trashdetectionv8...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Attempting to download version 2 of trashdetectionv8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/trashdetectionv8 to yolov8:: 100%|██████████| 79356/79356 [00:05<00:00, 14662.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/trashdetectionv8 in yolov8:: 100%|██████████| 2168/2168 [00:00<00:00, 5712.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded trashdetectionv8 to /content/datasets/trashdetectionv8\n",
            "Contents of /content/datasets/trashdetectionv8: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Processing garbage_best...\n",
            "\rloading Roboflow workspace...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rloading Roboflow project...\n",
            "Attempting to download version 1 of garbage_best\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/garbage_best to yolov8:: 100%|██████████| 177539/177539 [00:12<00:00, 14180.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/garbage_best in yolov8:: 100%|██████████| 4978/4978 [00:00<00:00, 5992.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded garbage_best to /content/datasets/garbage_best\n",
            "Contents of /content/datasets/garbage_best: ['README.dataset.txt', 'README.roboflow.txt', 'test', 'train', 'data.yaml', 'valid']\n",
            "--------------------------------------------------\n",
            "Download process completed. Please check the following directory for your files:\n",
            "/content/datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "rf = Roboflow(api_key=\"TZCi2SCltMQkbaVAtOqT\")\n",
        "\n",
        "# List of datasets to download (workspace, dataset, version)\n",
        "datasets_info = [\n",
        "    (\"hongmo\", \"sea-ezx3q\", 44),\n",
        "    (\"kim-borma\", \"marine-debris-vb2qm\", 16),\n",
        "    (\"myfyp\", \"new-taco\", 3),\n",
        "    (\"anas-fd5bu\", \"-kg\", 5),\n",
        "    (\"trash-detection-eka7g\", \"aeriel-trash-detection\", 5),\n",
        "    (\"fyp-bfx3h\", \"yolov8-trash-detections\", 6),\n",
        "    (\"taco-ihjgk\", \"yolov8-trash-detections-kgnug\", 11),\n",
        "    (\"trash-ai-27vay\", \"ultimate-rqkdd\", 1),\n",
        "    (\"iqram\", \"trashdetectionv8\", 2),\n",
        "    (\"smart-india-hackathon-2023\", \"garbage_best\", 1)\n",
        "]\n",
        "\n",
        "dest_dir = os.path.abspath(\"datasets\")\n",
        "print(f\"Destination directory: {dest_dir}\")\n",
        "\n",
        "if not os.path.exists(dest_dir):\n",
        "    os.makedirs(dest_dir)\n",
        "\n",
        "for workspace, dataset_name, version in datasets_info:\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "    try:\n",
        "        project = rf.workspace(workspace).project(dataset_name)\n",
        "        print(f\"Attempting to download version {version} of {dataset_name}\")\n",
        "\n",
        "        # Specify the download location\n",
        "        download_dir = os.path.join(dest_dir, dataset_name)\n",
        "\n",
        "        # Download the dataset to the specified location\n",
        "        dataset = project.version(version).download(\"yolov8\", location=download_dir)\n",
        "        print(f\"Successfully downloaded {dataset_name} to {dataset.location}\")\n",
        "\n",
        "        # List the contents of the downloaded directory\n",
        "        if os.path.exists(dataset.location):\n",
        "            print(f\"Contents of {dataset.location}: {os.listdir(dataset.location)}\")\n",
        "        else:\n",
        "            print(f\"Warning: Downloaded directory not found at {dataset.location}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dataset_name}: {str(e)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Download process completed. Please check the following directory for your files:\")\n",
        "print(dest_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import glob\n",
        "\n",
        "def merge_datasets(datasets_dir, combined_dataset_dir):\n",
        "    # Create directories for the combined dataset\n",
        "    os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "    splits = ['train', 'valid', 'test']\n",
        "    for split in splits:\n",
        "        os.makedirs(os.path.join(combined_dataset_dir, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(combined_dataset_dir, 'labels', split), exist_ok=True)\n",
        "\n",
        "    combined_classes = []\n",
        "    class_mappings = {}\n",
        "\n",
        "    # First pass: Collect all class names and build combined_classes list\n",
        "    for dataset_name in os.listdir(datasets_dir):\n",
        "        dataset_path = os.path.join(datasets_dir, dataset_name)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "        if not os.path.exists(data_yaml_path):\n",
        "            print(f\"data.yaml not found in {dataset_path}. Skipping this dataset.\")\n",
        "            continue\n",
        "\n",
        "        with open(data_yaml_path, 'r') as f:\n",
        "            data_yaml = yaml.safe_load(f)\n",
        "\n",
        "        dataset_classes = data_yaml.get('names', [])\n",
        "        if not dataset_classes:\n",
        "            print(f\"No 'names' field found in {data_yaml_path}. Skipping this dataset.\")\n",
        "            continue\n",
        "\n",
        "        # Update combined_classes and create class mappings for this dataset\n",
        "        mapping = {}\n",
        "        for idx, class_name in enumerate(dataset_classes):\n",
        "            if class_name not in combined_classes:\n",
        "                combined_classes.append(class_name)\n",
        "            mapping[idx] = combined_classes.index(class_name)\n",
        "        class_mappings[dataset_name] = mapping\n",
        "\n",
        "    print(f\"Combined classes: {combined_classes}\")\n",
        "\n",
        "    # Second pass: Copy images and labels, remap class IDs\n",
        "    for dataset_name in os.listdir(datasets_dir):\n",
        "        dataset_path = os.path.join(datasets_dir, dataset_name)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        mapping = class_mappings.get(dataset_name)\n",
        "        if mapping is None:\n",
        "            continue  # Skip datasets that were not included in combined_classes\n",
        "\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            split_dir = os.path.join(dataset_path, split)\n",
        "            if not os.path.exists(split_dir):\n",
        "                print(f\"Split '{split}' not found in {dataset_name}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Check if images and labels are under 'images' and 'labels' directories\n",
        "            images_dir = os.path.join(split_dir, 'images')\n",
        "            labels_dir = os.path.join(split_dir, 'labels')\n",
        "            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
        "                # Maybe images and labels are directly under split_dir\n",
        "                images_dir = split_dir\n",
        "                labels_dir = split_dir\n",
        "                # Check if there are images and labels in split_dir\n",
        "                image_files = glob.glob(os.path.join(images_dir, '*.jpg')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.jpeg')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.png')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.bmp')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.tif')) + \\\n",
        "                              glob.glob(os.path.join(images_dir, '*.tiff'))\n",
        "                label_files = glob.glob(os.path.join(labels_dir, '*.txt'))\n",
        "                if len(image_files) == 0:\n",
        "                    print(f\"No images found in {images_dir}. Skipping split '{split}' in dataset '{dataset_name}'.\")\n",
        "                    continue\n",
        "            else:\n",
        "                # Images and labels are under 'images' and 'labels' directories\n",
        "                image_files = glob.glob(os.path.join(images_dir, '*.*'))\n",
        "                label_files = glob.glob(os.path.join(labels_dir, '*.txt'))\n",
        "\n",
        "            for image_file in image_files:\n",
        "                image_filename = os.path.basename(image_file)\n",
        "                image_name, image_ext = os.path.splitext(image_filename)\n",
        "\n",
        "                # Corresponding label file\n",
        "                label_file = os.path.join(labels_dir, f\"{image_name}.txt\")\n",
        "                if not os.path.exists(label_file):\n",
        "                    print(f\"Label file {label_file} not found. Creating empty label.\")\n",
        "                    label_lines = []\n",
        "                else:\n",
        "                    with open(label_file, 'r') as lf:\n",
        "                        label_lines = lf.readlines()\n",
        "\n",
        "                # Remap class IDs in label file\n",
        "                remapped_labels = []\n",
        "                for line in label_lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:\n",
        "                        print(f\"Invalid label format in {label_file}. Skipping line.\")\n",
        "                        continue\n",
        "                    try:\n",
        "                        class_id = int(parts[0])\n",
        "                        new_class_id = mapping.get(class_id)\n",
        "                        if new_class_id is None:\n",
        "                            print(f\"Class ID {class_id} not found in mapping for {dataset_name}. Skipping line.\")\n",
        "                            continue\n",
        "                        parts[0] = str(new_class_id)\n",
        "                        remapped_labels.append(' '.join(parts))\n",
        "                    except ValueError:\n",
        "                        print(f\"Invalid class ID in {label_file}. Skipping line.\")\n",
        "                        continue\n",
        "\n",
        "                # Copy image\n",
        "                dest_image_name = f\"{dataset_name}_{image_filename}\"\n",
        "                target_split = split  # Use the same split name\n",
        "                dest_image_path = os.path.join(combined_dataset_dir, 'images', target_split, dest_image_name)\n",
        "                shutil.copy(image_file, dest_image_path)\n",
        "\n",
        "                # Write remapped label\n",
        "                dest_label_name = f\"{dataset_name}_{image_name}.txt\"\n",
        "                dest_label_path = os.path.join(combined_dataset_dir, 'labels', target_split, dest_label_name)\n",
        "                with open(dest_label_path, 'w') as lf:\n",
        "                    lf.write('\\n'.join(remapped_labels))\n",
        "\n",
        "    # Create combined data.yaml\n",
        "    combined_data = {\n",
        "        'path': os.path.abspath(combined_dataset_dir),\n",
        "        'train': os.path.join('images', 'train'),\n",
        "        'val': os.path.join('images', 'valid'),  # Use 'valid' as validation set\n",
        "        'names': combined_classes\n",
        "    }\n",
        "    with open(os.path.join(combined_dataset_dir, 'data.yaml'), 'w') as f:\n",
        "        yaml.dump(combined_data, f)\n",
        "\n",
        "    print(\"Dataset merging complete.\")\n",
        "\n",
        "# Usage\n",
        "datasets_dir = 'datasets'  # Directory containing individual datasets\n",
        "combined_dataset_dir = 'combined_dataset'  # Directory to store the combined dataset\n",
        "\n",
        "merge_datasets(datasets_dir, combined_dataset_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01G3t6pRx__z",
        "outputId": "3cf610b0-834f-4662-cd55-7a95d36a1d8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined classes: ['battery', 'can', 'cardboard', 'drink carton', 'glass bottle', 'paper', 'plastic bag', 'plastic bottle', 'plastic bottle cap', 'pop tab', '0', '1', '2', 'Can', 'Cup', 'Food Waste', 'Mask', 'Needle', 'Plastic Bottle', 'Trash -everything else-', 'plastic-paper scraps', 'fabrics', 'glass', 'metal', 'other', 'plastic', 'wood', 'aluminum', 'cap or lid', 'cardboard boxes and cartons', 'food - others', 'plastic bag or wrapper', 'plastic container', 'styrofoam', 'utensils and straw', 'PET', 'Styrofoam', 'Decomposable', 'Glass', 'Metal', 'Non-decomposable', 'Non-decomposable-', 'Plastic', 'Net', 'PET_Bottle', 'Plastic_Buoy', 'Plastic_Buoy_China', 'Plastic_ETC', 'Rope', 'Styrofoam_Box', 'Styrofoam_Buoy', 'Styrofoam_Piece', 'Aluminium foil', 'Bottle', 'Bottle cap', 'Carton', 'Cigarette', 'Glass bottle', 'Lid', 'Metal cap', 'Other litter', 'Other plastic', 'Paper', 'Plastic bag', 'Plastic buoy', 'Plastic vessels', 'Pop tab', 'Straw', 'Styrofoam cup', 'Styrofoam piece', 'Wrapper', 'c', 'garbage', 'garbage_bag', 'sampah-detection', 'trash']\n",
            "Split 'test' not found in yolov8-trash-detections. Skipping.\n",
            "Dataset merging complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Set wandb project and run names\n",
        "wandb_project_name = 'Edutech'\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    config={\n",
        "        'epochs': 5,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 0.001,\n",
        "        # Add other hyperparameters if needed\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Load the YOLOv8 extra-large detection model\n",
        "model = YOLO('yolov8x.pt')  # Detection model for object detection tasks\n",
        "\n",
        "# Path to the combined dataset's data.yaml\n",
        "data_yaml_path = os.path.join('combined_dataset', 'data.yaml')\n",
        "\n",
        "# Verify that data.yaml exists\n",
        "if not os.path.exists(data_yaml_path):\n",
        "    raise FileNotFoundError(f\"Error: data.yaml not found at {data_yaml_path}\")\n",
        "else:\n",
        "    print(f\"Using data.yaml at {data_yaml_path}\")\n",
        "\n",
        "# Train the model with wandb logging enabled\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    # name=wandb_run_name,\n",
        "    exist_ok=True,\n",
        "    device='0',\n",
        "    project=wandb_project_name,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pljuUYoix_8Y",
        "outputId": "48521492-5317-4abf-bc26-cc400248de4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshng2025\u001b[0m (\u001b[33mmarlborough-college-malaysia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241016_164306-q1s09o0f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/Edutech/runs/q1s09o0f' target=\"_blank\">classic-donkey-1</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/Edutech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/Edutech' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/Edutech</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/Edutech/runs/q1s09o0f' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/Edutech/runs/q1s09o0f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using data.yaml at combined_dataset/data.yaml\n",
            "Ultralytics 8.3.14 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=combined_dataset/data.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=Edutech, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=Edutech/train\n",
            "Overriding model.yaml nc=80 with nc=76\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1   2265716  ultralytics.nn.modules.head.Detect           [76, [320, 640, 640]]         \n",
            "Model summary: 389 layers, 61,700,356 parameters, 61,700,340 gradients, 227.9 GFLOPs\n",
            "\n",
            "Transferred 553/631 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Edutech/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined_dataset/labels/train.cache... 95101 images, 16062 backgrounds, 0 corrupt: 100%|██████████| 95101/95101 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/-kg_frame_3069_patch_0_0_jpg.rf.11338fd82fcb4c2d2976c7d82b39339d.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/-kg_renamed_DJI_0471_tile_3_7_jpg.rf.987a8f6df1d8eb7780053f92d712e45e.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/ultimate-rqkdd_c5c0b71f-6c22-488d-9eb1-afb9d1ddd395_jpg.rf.83b01a0adaed207137d48d8623d3aaa4.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/ultimate-rqkdd_c5c0b71f-6c22-488d-9eb1-afb9d1ddd395_jpg.rf.abafe2f6d89502c7e77eb7914205151c.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/ultimate-rqkdd_c5c0b71f-6c22-488d-9eb1-afb9d1ddd395_jpg.rf.eafc6b47b5a00d1f2fd1150065ef272a.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/yolov8-trash-detections_20231217_150330_jpg.rf.0bd7a77c8550f78cdcda8ad68a2cf208.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/combined_dataset/images/train/yolov8-trash-detections_20231218_002501_jpg.rf.05629e8bba0bb702f17846205f9891a0.jpg: 1 duplicate labels removed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 39132, len(boxes) = 233397. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined_dataset/labels/valid.cache... 10761 images, 696 backgrounds, 0 corrupt: 100%|██████████| 10761/10761 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 4775, len(boxes) = 30715. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to Edutech/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000125, momentum=0.9) with parameter groups 103 weight(decay=0.0), 110 weight(decay=0.0005), 109 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mEdutech/train\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5      17.1G      1.327      5.481      1.545         56        640:   5%|▍         | 285/5944 [03:31<1:09:48,  1.35it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference using the best model\n",
        "results = model.predict(\n",
        "    source=os.path.join(combined_dataset_dir, 'images/val'),\n",
        "    save=True,\n",
        "    project='runs/predict',\n",
        "    name='yolov8x_predict',\n",
        "    exist_ok=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "cuxYBebkx_wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def visualize_predictions(result_dir):\n",
        "    image_paths = glob.glob(os.path.join(result_dir, '*.jpg'))\n",
        "    num_images = min(4, len(image_paths))\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i, image_path in enumerate(image_paths[:num_images]):\n",
        "        image = plt.imread(image_path)\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions('runs/predict/yolov8x_predict')\n"
      ],
      "metadata": {
        "id": "87daXwzIyLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOU1jsLvyLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21RTNNTlyLiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFCIHyOGyLfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}